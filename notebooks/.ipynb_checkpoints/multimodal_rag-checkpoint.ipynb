{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Chroma Storage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "# Create Chroma\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"ciam_rag_data\", \n",
    "    embedding_function=OpenCLIPEmbeddings(model=None, preprocess=None, tokenizer=None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "product_path = \"../training-sets/MEN_FASHION_WITH_REVIEWS\"\n",
    "product_list = json.loads(Path(product_path + \"/raw_products.json\").read_text())\n",
    "products = product_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "for prod in products:\n",
    "    keys = [\n",
    "        \"reviews\",\n",
    "        \"category\",\n",
    "        \"price\",\n",
    "        \"currency\",\n",
    "        \"total_customers_that_rated\",\n",
    "        \"overall_ratings\",\n",
    "        \"description\",\n",
    "        \"name\",\n",
    "        \"id\",\n",
    "        \"description\",\n",
    "        \"product_asin\"\n",
    "    ]\n",
    "    product_asin = prod['product_asin']\n",
    "    print(Path(f\"{product_path}/images/{product_asin}.png\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Document for each product\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def convert_to_document(product: dict):\n",
    "    product_asin = product.get('product_asin').strip()\n",
    "    product_name = product.get('name').strip()\n",
    "    price_currency = product.get('currency')\n",
    "    product_description = product.get('description').strip()\n",
    "    image_path = str(Path(f\"{product_path}/images/{product_asin}.png\").absolute())\n",
    "    document = Document(\n",
    "        id=product_asin,\n",
    "        page_content=f\"Product Asin: {product_asin} \\n\\n Name: {product_name} \\n\\n Description: {product_description}\",\n",
    "        metadata={\n",
    "            \"category\": product.get(\"category\"),\n",
    "            \"product_name\": product_name,\n",
    "            \"price\": f\"{price_currency}{product.get('price')}\",\n",
    "            \"overall_ratings\": product.get(\"overall_ratings\"),\n",
    "            \"product_asin\": product.get(\"product_asin\"),\n",
    "            \"total_customers_that_rated\": product.get('total_customers_that_rated'),\n",
    "            \"image_path\": image_path,\n",
    "        }\n",
    "    )\n",
    "    return document\n",
    "\n",
    "\n",
    "documents = list(map(convert_to_document, product_list[0:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VectorStore.search() missing 1 required positional argument: 'search_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m image_doc_ids \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39madd_images( uris\u001b[38;5;241m=\u001b[39m[ image[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m document_images ], metadata\u001b[38;5;241m=\u001b[39mdocument_images, ids\u001b[38;5;241m=\u001b[39m[ image_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_asin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m image_data \u001b[38;5;129;01min\u001b[39;00m document_images ] )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m doc_ids \u001b[38;5;241m==\u001b[39m image_doc_ids\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msneakers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     18\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "\u001b[0;31mTypeError\u001b[0m: VectorStore.search() missing 1 required positional argument: 'search_type'"
     ]
    }
   ],
   "source": [
    "# Add our data into the Vector Store\n",
    "doc_ids = vectorstore.add_documents(documents, ids=[doc.id for doc in documents])\n",
    "\n",
    "document_images = [\n",
    "    {\n",
    "        \"url\": doc.metadata.get(\"image_path\"),\n",
    "        \"product_asin\": doc.metadata.get(\"product_asin\"),\n",
    "        \"product_name\": doc.metadata.get(\"name\"),\n",
    "    } for doc in documents\n",
    "]\n",
    "\n",
    "image_doc_ids = vectorstore.add_images( uris=[ image['url'] for image in document_images ], metadata=document_images, ids=[ image_data['product_asin'] for image_data in document_images ] )\n",
    "assert doc_ids == image_doc_ids\n",
    "\n",
    "result = vectorstore.search(\"sneakers\", \"similarity\")\n",
    "print(result)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions from Langchain cookbooks\n",
    "\n",
    "import base64\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_base63_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string.\n",
    "    \n",
    "    Args:\n",
    "    base64_string (str): Base64 string of the original image size (tuple): Desired size of the image as (width, height).\n",
    "    \n",
    "    Returns:\n",
    "    str: Base64 string of the resized image.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "    \n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def is_base64(s):\n",
    "    \"\"\"Check if a string is Base64 encoded\"\"\"\n",
    "    \n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"Split numpy arrays images and texts\"\"\"\n",
    "    \n",
    "    images = []\n",
    "    texts = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        converted_doc = doc.page_content # Extract Document contents\n",
    "        if is_base64(converted_doc):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(\n",
    "                resize_base63_image(converted_doc, size=(250, 250))\n",
    "            ) # base64 encoded str\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "            \n",
    "    return {\"images\": images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'Fashion', 'image_path': '/home/solomon/Documents/projects/ciam2rag/ciam2rag_core/notebooks/../training-sets/MEN_FASHION_WITH_REVIEWS/images/B01A6LTSUK.png', 'overall_ratings': 4.7, 'price': '£38.49', 'product_asin': 'B01A6LTSUK', 'total_customers_that_rated': 102877}\n",
      "{'category': 'Fashion', 'image_path': '/home/solomon/Documents/projects/ciam2rag/ciam2rag_core/notebooks/../training-sets/MEN_FASHION_WITH_REVIEWS/images/B01A6LTSUK.png', 'overall_ratings': 4.7, 'price': '£38.49', 'product_asin': 'B01A6LTSUK', 'total_customers_that_rated': 102877}\n",
      "{'category': 'Fashion', 'image_path': '/home/solomon/Documents/projects/ciam2rag/ciam2rag_core/notebooks/../training-sets/MEN_FASHION_WITH_REVIEWS/images/B01A6LTSUK.png', 'overall_ratings': 4.7, 'price': '£38.49', 'product_asin': 'B01A6LTSUK', 'total_customers_that_rated': 102877}\n",
      "{'category': 'Fashion', 'image_path': '/home/solomon/Documents/projects/ciam2rag/ciam2rag_core/notebooks/../training-sets/MEN_FASHION_WITH_REVIEWS/images/B01A6LTSUK.png', 'overall_ratings': 4.7, 'price': '£38.49', 'product_asin': 'B01A6LTSUK', 'total_customers_that_rated': 102877}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    \n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "    \n",
    "    \n",
    "docs = retriever.invoke(\"shoe\", k=10)\n",
    "for doc in docs:\n",
    "    if is_base64(doc.page_content):\n",
    "        plt_img_base64(doc.page_content)\n",
    "    else:\n",
    "        print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "VISION_MODEL_NAME = \"gpt-4o\"\n",
    "# VISION_MODEL_NAME = \"llava\"\n",
    "VISION_MODEL_NAME = \"llama3\"\n",
    "\n",
    "OLLAMA_IN_USE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_func(data_dict: dict):\n",
    "    # Joining the context texts into a single string\n",
    "    \n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        print(\"Adding images to the messages\")\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"{data_dict['context']['images'][0]}\" if OLLAMA_IN_USE == \"llava\" else f\"data:image/jpeg;base64, {data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "    \n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You answer should be in the format \"\n",
    "            \"Product ASIN\\nProduct Name\\nYour Thought\\nPrice\\nOther Details\"\n",
    "            \"You work in a fashion store here in the UK. Your task as an intelligent customer \"\n",
    "            \"assistant is to answer the customer's query \"\n",
    "            \"You should also look at reviews to back up your answers.\"\n",
    "            \"The following are the products we pulled from the store that might \"\n",
    "            \"match their query, use this to answer their question: {context}\"\n",
    "            f\"User Query: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        )\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    \n",
    "    return [HumanMessage(content=messages)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OLLAMA_IN_USE:\n",
    "    from langchain_community.chat_models.ollama import ChatOllama\n",
    "    model = ChatOllama(model=VISION_MODEL_NAME, temperature=0, \n",
    "                    num_ctx=4096, ## Max Tokens: 4096 for Ollama local models please\n",
    "            )\n",
    "else:\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    { \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough()\n",
    "     }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, Document found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the best material to wear during winter?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:2796\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2795\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2796\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:4359\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4346\u001b[0m \n\u001b[1;32m   4347\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4356\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4369\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:1734\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1730\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1731\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1732\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1733\u001b[0m         Output,\n\u001b[0;32m-> 1734\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1737\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1742\u001b[0m     )\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1744\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py:379\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    378\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:4215\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4213\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4218\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py:379\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    378\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 4\u001b[0m, in \u001b[0;36mprompt_func\u001b[0;34m(data_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_func\u001b[39m(data_dict: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Joining the context texts into a single string\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     formatted_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Adding image(s) to the messages if present\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, Document found"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"What is the best material to wear during winter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "def get_conversation_chain():\n",
    "    \"\"\"Returns a chain that asks the model to generate an answer based on the given context and user summarised conversation.\n",
    "\n",
    "    Args:\n",
    "        vector_store (Chroma): The vectorstore that contains the documents\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A LargeChain Runnable\n",
    "    \"\"\"\n",
    "\n",
    "    # llm = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=500)\n",
    "    llm = ChatOllama(temperature=0, model=VISION_MODEL_NAME)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", CUSTOMER_SUPPORT_EXPERT_INTRODUCTORY_INSTRUCTION),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "    return create_retrieval_chain(\n",
    "        retriever=get_retrieval_chain(vector_store=vector_store),\n",
    "        combine_docs_chain=stuff_documents_chain,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_retrieval_chain(retriever):\n",
    "    \"\"\"Retrieves the document that matches the query of the user. The function uses the `create_history_aware_retriever`\n",
    "    function from LangChain to collate the conversations we have been having with this user, summarises it and use the summary\n",
    "    to find relevant documents in the vector store.\n",
    "\n",
    "    Args:\n",
    "        vector_store (Chroma): The vector that contains the documents.\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A LargeChain Runnable\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOllama(temperature=0, model=\"llama3\")\n",
    "\n",
    "    # Using the normal retriever technique, as our use case is simple.\n",
    "    # Using other retriever could cause slight delay in responding to the\n",
    "    # user as they will need to use LLM to perform various tasks.\n",
    "    # Our use-case is simple and using this retriever works most (if not all) the time.\n",
    "    # retriever\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            SystemMessage(content=\"As an expert in a Fashion Store, given the above conversation, generate a search query to look up in order to get information relevant to the  conversation\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return create_history_aware_retriever(llm=llm, retriever=retriever, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = get_conversation_chain()\n",
    "assistant_response = conversation_chain.invoke(\n",
    "    {\"input\": \"I need a sneakers\", \"chat_history\": [] }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
